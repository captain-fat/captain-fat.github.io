<!DOCTYPE html>
<html lang="en">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/apple-touch-icon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="description" content="A Place where You Can Know about Me">
  <meta name="author" content="Yifan">
  <meta name="keywords" content="">
  <title>python爬虫一：分析豆瓣中正在上映的电影短评 ~ Yifan&#39;s Place</title>

  <link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"  >
<link rel="stylesheet" href="/lib/bootstrap/css/bootstrap.min.css"  >
<link rel="stylesheet" href="/lib/mdbootstrap/css/mdb.min.css"  >
<link rel="stylesheet" href="/lib/github-markdown/github-markdown.min.css"  >

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css"  >

<link rel="stylesheet" href="/css/main.css"  >


  <link rel="stylesheet" href="/lib/fancybox/jquery.fancybox.min.css"  >


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Yifan's Place</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">Home</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">Archives</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">Categories</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">Tags</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">About</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" false
         style="background: url('/img/home_bgim.png')no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              <br>
              
                <p class="mt-3">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>&nbsp;
                  Friday, February 21st 2020, 1:33 pm
                </p>
              

              <p>
                
                  
                  &nbsp;<i class="far fa-chart-bar"></i>
                  <span class="post-count">
                    1.9k 字
                  </span>&nbsp;
                

                

                
                  <!-- 不蒜子统计文章PV -->
                  
                  &nbsp;<i class="far fa-eye" aria-hidden="true"></i>&nbsp;
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span> 次
                  </span>&nbsp;
                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="py-5 z-depth-3" id="board">
        <div class="post-content mx-auto" id="post">
          <div class="markdown-body">
            <p>用python写爬虫抓取豆瓣短评并进行词云展示</p>
<h1 id="主要目标"><a href="#主要目标" class="headerlink" title="主要目标"></a>主要目标</h1><p>主要做了三件事：</p>
<ul>
<li>抓取网页数据</li>
<li>清理数据</li>
<li>进行词云展示</li>
</ul>
<h1 id="抓取网页数据"><a href="#抓取网页数据" class="headerlink" title="抓取网页数据"></a>抓取网页数据</h1><p><strong>第一步要对网页进行访问，python中使用的是urllib库。代码如下：</strong></p>
<pre><code class="python">import urllib

url = &#39;https://movie.douban.com/cinema/nowplaying/suzhou/&#39;
head = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0&#39;}

request = urllib.request.Request(url = url, headers = head)
resp = urllib.request.urlopen(request)
html_data = resp.read().decode(&#39;utf-8&#39;)
print(html_data)</code></pre>
<p>其中<a href="https://movie.douban.com/cinema/nowplaying/suzhou/" target="_blank" rel="noopener">https://movie.douban.com/cinema/nowplaying/suzhou/</a> ，可以在浏览器中输入该网址进行查看。<br>html_data是字符串类型的变量，里面存放了网页的html代码。<br>输入print(html_data)可以查看，如下图所示：</p>
<p><img src="/images/pasted-2.png" srcset="/img/loading.gif" alt="图片1"></p>
<p><strong>第二步，需要对得到的html代码进行解析，从里面提取我们需要的数据。</strong><br>在python中使用BeautifulSoup库进行html代码的解析。<br><del>（注：如果没有安装此库，则使用pip install BeautifulSoup进行安装即可！）<br>BeautifulSoup使用的格式如下：</del><br>pip install BeautifulSoup4</p>
<pre><code class="python">BeautifulSoup(html,&quot;html.parser&quot;)</code></pre>
<p>第一个参数为需要提取数据的html，第二个参数是指定解析器，然后使用find_all()读取html标签中的内容.  </p>
<p>但是html中有这么多的标签，该读取哪些标签呢？其实，最简单的办法是可以打开我们爬取网页的html代码，然后查看我们需要的数据在哪个html标签里面，再进行读取就可以了。如下图所示：</p>
<p><img src="/images/pasted-3.png" srcset="/img/loading.gif" alt="图片2"><br>从上图中可以看出在<em>div id=”nowplaying”</em>标签开始是我们想要的数据，里面有电影的名称、评分、主演等信息。所以相应的代码编写如下：</p>
<pre><code class="python">from bs4 import BeautifulSoup as bs
soup = bs(html_data, &#39;html.parser&#39;)
nowplaying_movie = soup.find_all(&#39;div&#39;, id=&#39;nowplaying&#39;)
nowplaying_movie_list = nowplaying_movie[0].find_all(&#39;li&#39;, class_=&#39;list-item&#39;)</code></pre>
<pre><code class="python">nowplaying_list = [] 
for item in nowplaying_movie_list:        
        nowplaying_dict = {}        
        nowplaying_dict[&#39;id&#39;] = item[&#39;data-subject&#39;]       
        for tag_img_item in item.find_all(&#39;img&#39;):            
            nowplaying_dict[&#39;name&#39;] = tag_img_item[&#39;alt&#39;]            
            nowplaying_list.append(nowplaying_dict)  </code></pre>
<p><img src="/images/pasted-4.png" srcset="/img/loading.gif" alt="图片3"></p>
<p>可以看到和豆瓣网址上面是匹配的。这样就得到了最新电影的信息了。接下来就要进行对最新电影短评进行分析了。例如《战狼2》的短评网址为：<a href="https://movie.douban.com/subject/26363254/comments?start=0&amp;limit=20" target="_blank" rel="noopener">https://movie.douban.com/subject/26363254/comments?start=0&amp;limit=20</a><br>其中26363254就是电影的id，start=0表示评论的第0条评论。</p>
<p>接下来接对该网址进行解析了。打开上图中的短评页面的html代码，我们发现关于评论的数据是在div标签的comment属性下面，如下图所示：</p>
<p><img src="/images/pasted-5.png" srcset="/img/loading.gif" alt="图片4"></p>
<p>因此对此标签进行解析，代码如下：</p>
<pre><code class="python">requrl = &#39;https://movie.douban.com/subject/&#39; + nowplaying_list[0][&#39;id&#39;] + &#39;/comments&#39; +&#39;?&#39; +&#39;start=0&#39; + &#39;&amp;limit=20&#39;
resp = urllib.request.Request(url = requrl, headers = head)
resp = urllib.request.urlopen(resp)
html_data = resp.read().decode(&#39;utf-8&#39;)
soup = bs(html_data, &#39;html.parser&#39;)
comment_div_list = soup.find_all(&#39;div&#39;, class_ = &#39;comment&#39;)</code></pre>
<pre><code class="python">eachCommentList = []
for item in comment_div_list:
    if item.find_all(&#39;span&#39;, &#39;short&#39;)[0].string is not None:
        eachCommentList.append(item.find_all(&#39;span&#39;, &#39;short&#39;)[0].string)</code></pre>
<p><img src="/images/pasted-7.png" srcset="/img/loading.gif" alt="图片5"></p>
<p>好的，至此我们已经爬取了豆瓣最近播放电影的评论数据，接下来就要对数据进行清洗和词云显示了。</p>
<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><p>为了方便进行数据进行清洗，我们将列表中的数据放在一个字符串数组中，代码如下：</p>
<pre><code class="python">comments = &#39;&#39;
for k in range(len(eachCommentList)):
    comments = comments + (str(eachCommentList[k])).strip()</code></pre>
<p>使用print(comments)进行查看，如下图所示：</p>
<p><img src="/images/pasted-8.png" srcset="/img/loading.gif" alt="图片6"></p>
<p>可以看到所有的评论已经变成一个字符串了，但是我们发现评论中还有不少的标点符号等。这些符号对我们进行词频统计时根本没有用，因此要将它们清除。所用的方法是正则表达式。python中正则表达式是通过re模块来实现的。代码如下：</p>
<pre><code class="python">pattern = re.compile(r&#39;[\u4e00-\u9fa5]+&#39;)
filterdata = re.findall(pattern, comments)
cleaned_comments = &#39;&#39;.join(filterdata)</code></pre>
<p><img src="/images/pasted-9.png" srcset="/img/loading.gif" alt="7"><br>我们可以看到此时评论数据中已经没有那些标点符号了，数据变得“干净”了很多。</p>
<p>因此要进行词频统计，所以先要进行中文分词操作。在这里我使用的是结巴分词。如果没有安装结巴分词，可以在控制台使用pip install jieba进行安装。</p>
<pre><code class="python">import jieba
import pandas as pd

segment = jieba.lcut(cleaned_comments)
words_df = pd.DataFrame({&#39;segment&#39;:segment})</code></pre>
<p>因为结巴分词要用到pandas，所以我们这里加载了pandas包。可以使用words_df.head()查看分词之后的结果，如下图所示：</p>
<p><img src="/images/pasted-10.png" srcset="/img/loading.gif" alt="8"><br>从上图可以看到我们的数据中有“看”、“太”、“的”等虚词（停用词），而这些词在任何场景中都是高频词，并且没有实际的含义，所以我们要将他们清除。</p>
<p>我把停用词放在一个stopwords.txt文件中，将我们的数据与停用词进行比对即可（注：<a href="https://github.com/captain-fat/short_text_classification/tree/master/conf" target="_blank" rel="noopener">https://github.com/captain-fat/short_text_classification/tree/master/conf</a> ）<br>去停用词代码如下：</p>
<pre><code class="python">stopwords = pd.read_csv(&#39;chineseStopWords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;gb18030&#39;)#可能用&#39;utf-8&#39;
words_df = words_df[~words_df.segment.isin(stopwords.stopword)]</code></pre>
<p>继续使用words_df.head()语句来查看结果，如下图所示，停用词已经被除去了。<br><img src="/images/pasted-11.png" srcset="/img/loading.gif" alt="9"></p>
<p>接下来就要进行词频统计了，代码如下：</p>
<pre><code class="python">import numpy as np
words_stat = words_df.groupby(by=[&#39;segment&#39;])[&#39;segment&#39;].agg({&quot;计数&quot;:np.size})
words_stat = words_stat.reset_index().sort_values(by=[&#39;计数&#39;], ascending = False)</code></pre>
<blockquote>
<p>FutureWarning: using a dict on a Series for aggregation<br>is deprecated and will be removed in a future version. Use                 named aggregation instead.   </p>
<blockquote>
<blockquote>
<p>grouper.agg(name_1=func_1, name_2=func_2)<br>  “””Entry point for launching an IPython kernel.</p>
</blockquote>
</blockquote>
</blockquote>
<h1 id="词云展示"><a href="#词云展示" class="headerlink" title="词云展示"></a>词云展示</h1><p>代码如下：</p>
<pre><code class="python">import matplotlib.pyplot as plt
%matplotlib inline
import matplotlib
matplotlib.rcParams[&#39;figure.figsize&#39;] = (10.0, 5.0)
from wordcloud import WordCloud#词云包

wordcloud=WordCloud(font_path=&quot;simhei.ttf&quot;,background_color=&quot;white&quot;,max_font_size=80) #指定字体类型、字体大小和字体颜色
word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}

word_frequence_list = []
for key in word_frequence:
    temp = (key,word_frequence[key])
    word_frequence_list.append(temp)
word_frequence_list = dict(word_frequence_list)
wordcloud=wordcloud.fit_words(word_frequence_list)
plt.imshow(wordcloud)</code></pre>
<p><img src="/images/pasted-12.png" srcset="/img/loading.gif" alt="结果"></p>
<p>完整程序</p>
<pre><code class="python">import urllib
from bs4 import BeautifulSoup as bs
import re
import jieba
import pandas as pd
import numpy as np

url = &#39;https://movie.douban.com/cinema/nowplaying/suzhou/&#39;
head = {&#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:73.0) Gecko/20100101 Firefox/73.0&#39;}

request = urllib.request.Request(url = url, headers = head)
resp = urllib.request.urlopen(request)
html_data = resp.read().decode(&#39;utf-8&#39;)
print(html_data)

soup = bs(html_data, &#39;html.parser&#39;)
nowplaying_movie = soup.find_all(&#39;div&#39;, id=&#39;nowplaying&#39;)
nowplaying_movie_list = nowplaying_movie[0].find_all(&#39;li&#39;, class_=&#39;list-item&#39;)

nowplaying_list = []
for item in nowplaying_movie_list:
    nowplaying_dict = {}
    nowplaying_dict[&#39;id&#39;] = item[&#39;data-subject&#39;]
    for tag_img_item in item.find_all(&#39;img&#39;):
        nowplaying_dict[&#39;name&#39;] = tag_img_item[&#39;alt&#39;]
        nowplaying_list.append(nowplaying_dict)

requrl = &#39;https://movie.douban.com/subject/&#39; + nowplaying_list[0][&#39;id&#39;] + &#39;/comments&#39; +&#39;?&#39; +&#39;start=0&#39; + &#39;&amp;limit=20&#39;
resp = urllib.request.Request(url = requrl, headers = head)
resp = urllib.request.urlopen(resp)
html_data = resp.read().decode(&#39;utf-8&#39;)
soup = bs(html_data, &#39;html.parser&#39;)
comment_div_list = soup.find_all(&#39;div&#39;, class_ = &#39;comment&#39;)

eachCommentList = []
for item in comment_div_list:
    if item.find_all(&#39;span&#39;, &#39;short&#39;)[0].string is not None:
        eachCommentList.append(item.find_all(&#39;span&#39;, &#39;short&#39;)[0].string)

comments = &#39;&#39;
for k in range(len(eachCommentList)):
    comments = comments + (str(eachCommentList[k])).strip()

pattern = re.compile(r&#39;[\u4e00-\u9fa5]+&#39;)
filterdata = re.findall(pattern, comments)
cleaned_comments = &#39;&#39;.join(filterdata)

segment = jieba.lcut(cleaned_comments)
words_df = pd.DataFrame({&#39;segment&#39;:segment})

stopwords = pd.read_csv(&#39;chineseStopWords.txt&#39;,index_col=False,quoting=3,sep=&quot;\t&quot;,names=[&#39;stopword&#39;], encoding=&#39;gb18030&#39;)
words_df = words_df[~words_df.segment.isin(stopwords.stopword)]

words_stat = words_df.groupby(by=[&#39;segment&#39;])[&#39;segment&#39;].agg({&quot;计数&quot;:np.size})
words_stat = words_stat.reset_index().sort_values(by=[&#39;计数&#39;], ascending = False)

import matplotlib.pyplot as plt
import matplotlib
matplotlib.rcParams[&#39;figure.figsize&#39;] = (10.0, 5.0)
from wordcloud import WordCloud#词云包

wordcloud=WordCloud(font_path=&quot;simhei.ttf&quot;,background_color=&quot;white&quot;,max_font_size=80) #指定字体类型、字体大小和字体颜色
word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}

word_frequence_list = []
for key in word_frequence:
    temp = (key,word_frequence[key])
    word_frequence_list.append(temp)
word_frequence_list = dict(word_frequence_list)
wordcloud=wordcloud.fit_words(word_frequence_list)
plt.imshow(wordcloud)</code></pre>
<p>参考：<br><a href="https://segmentfault.com/a/1190000010473819" target="_blank" rel="noopener">https://segmentfault.com/a/1190000010473819</a></p>

            <hr>
          </div>
          <br>
          <div>
            <p>
            
              <span>
                <i class="iconfont icon-inbox"></i>
                
                  <a class="hover-with-bg" href="/categories/%E9%95%BF%E7%9F%A5%E8%AF%86">长知识</a>
                  &nbsp;
                
              </span>&nbsp;&nbsp;
            
            
              <span>
                <i class="iconfont icon-tag"></i>
                
                  <a class="hover-with-bg" href="/tags/python">python</a>
                
              </span>
            
            </p>
            
              <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
            
          </div>
        </div>
      </div>
    </div>
    <div class="d-none d-lg-block col-lg-2 toc-container">
      
  <div id="toc">
    <p class="h4"><i class="far fa-list-alt"></i>&nbsp;TOC</p>
    <div id="tocbot"></div>
  </div>

    </div>
  </div>
</div>

<!-- custom -->


<!-- Comments -->
<div class="col-lg-7 mx-auto nopadding-md">
  <div class="container comments mx-auto" id="comments">
    
      <br><br>
      
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    <script>
      var disqus_config = function () {
        this.page.url = 'http://yifan.press/2020/02/21/长知识/python爬虫实战一：分析豆瓣中最新电影的影评/';
        this.page.identifier = '/2020/02/21/长知识/python爬虫实战一：分析豆瓣中最新电影的影评/';
      };
      var oldLoad = window.onload;
      window.onload = function () {
        var d = document, s = d.createElement('script');
        s.type = 'text/javascript';
        s.src = '//' + 'visitor' + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
      };
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" target="_blank" rel="nofollow noopener noopener">comments
        powered by Disqus.</a></noscript>
  </div>


    
  </div>
</div>

    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  <footer class="mt-5">
  <div class="text-center py-3">
    <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
    <i class="iconfont icon-love"></i>
    <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    <br>

    
  
    <!-- 不蒜子统计PV -->
    
    &nbsp;<span id="busuanzi_container_site_pv"></span>总访问量 
          <span id="busuanzi_value_site_pv"></span> 次&nbsp;
  
  
    <!-- 不蒜子统计UV -->
    
    &nbsp;<span id="busuanzi_container_site_uv"></span>总访客数 
            <span id="busuanzi_value_site_uv"></span> 人&nbsp;
  
  <br>



    


    <!-- cnzz Analytics icon -->
    

  </div>
</footer>

<!-- SCRIPTS -->
<script src="/lib/jquery/jquery.min.js" ></script>
<script src="/lib/popper/popper.min.js" ></script>
<script src="/lib/bootstrap/js/bootstrap.min.js" ></script>
<script src="/lib/mdbootstrap/js/mdb.min.js" ></script>
<script src="/js/main.js" ></script>


  <script src="/js/lazyload.js" ></script>



  
    <script src="/lib/tocbot/tocbot.min.js" ></script>
  
  <script src="/js/post.js" ></script>



  <script src="/lib/smoothscroll/SmoothScroll.min.js" ></script>



  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>


<!-- Plugins -->


  

  

  

  

  <!-- cnzz Analytics -->
  



  <script src="/lib/prettify/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script src="/lib/typed/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "python爬虫一：分析豆瓣中正在上映的电影短评&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script src="/lib/anchor/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "false",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script src="/lib/fancybox/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>












<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
